{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8fd488ed-1e11-4630-bed0-5d4318504f74",
   "metadata": {},
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb11e56-013a-4b47-85f3-2fff639eead9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Western Chalukya architecture, also known as Kalyani Chalukya or Later Chalukya architecture and broadly classified under the Vesara Style, is the distinctive style of ornamented architecture that evolved during the rule of the Western Chalukya Empire in the Tungabhadra region of modern central Karnataka, India, during '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=\"Western Chalukya architecture, also known as Kalyani Chalukya or Later Chalukya architecture and broadly classified under the Vesara Style, is the distinctive style of ornamented architecture that evolved during the rule of the Western Chalukya Empire in the Tungabhadra region of modern central Karnataka, India, during \"\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305b6bd-6f57-4b81-a169-7cd6ff51ab1f",
   "metadata": {},
   "source": [
    "# Tokenization >> means paragraph se sentence me convert krna ya sentence se word me convert krna  etc krna esi ko Tokenization bolte hai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9839bf5d-559d-4ce5-a68c-48472b0b9512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rk318\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\metrics\\association.py:26: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.3)\n",
      "  from scipy.stats import fisher_exact\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Western Chalukya architecture, also known as Kalyani Chalukya or Later Chalukya architecture and broadly classified under the Vesara Style, is the distinctive style of ornamented architecture that evolved during the rule of the Western Chalukya Empire in the Tungabhadra region of modern central Karnataka, India, during']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize  # sentence tokenization \n",
    "\n",
    "\n",
    "# ye sab pretrain hota haai \n",
    "\n",
    "\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d789183a-b5a0-407c-b9af-9a9cd8865eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Western Chalukya architecture, also known as Kalyani Chalukya or Later Chalukya architecture and broadly classified under the Vesara Style, is the distinctive style of ornamented architecture that evolved during the rule of the Western Chalukya Empire in the Tungabhadra region of modern central Karnataka, India, during\n"
     ]
    }
   ],
   "source": [
    "for i in sent_tokenize(corpus):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a7325-a555-4831-9252-3eadc7808e16",
   "metadata": {},
   "source": [
    "# ab sentence to word me convert krenge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a73adae2-4abb-4677-928e-178eb1cc2858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rahul',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'bay',\n",
       " 'Ramu',\n",
       " 'rita',\n",
       " 'ranu',\n",
       " 'sima',\n",
       " 'rampal',\n",
       " 'raju']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize   # word tokenizer \n",
    "word_tokenize(\"Rahul is a good bay Ramu rita ranu sima rampal raju \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305add97-34f5-4b3d-bf17-0351a4327846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Western',\n",
       " 'Chalukya',\n",
       " 'architecture',\n",
       " ',',\n",
       " 'also',\n",
       " 'known',\n",
       " 'as',\n",
       " 'Kalyani',\n",
       " 'Chalukya',\n",
       " 'or',\n",
       " 'Later',\n",
       " 'Chalukya',\n",
       " 'architecture',\n",
       " 'and',\n",
       " 'broadly',\n",
       " 'classified',\n",
       " 'under',\n",
       " 'the',\n",
       " 'Vesara',\n",
       " 'Style',\n",
       " ',',\n",
       " 'is',\n",
       " 'the',\n",
       " 'distinctive',\n",
       " 'style',\n",
       " 'of',\n",
       " 'ornamented',\n",
       " 'architecture',\n",
       " 'that',\n",
       " 'evolved',\n",
       " 'during',\n",
       " 'the',\n",
       " 'rule',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Western',\n",
       " 'Chalukya',\n",
       " 'Empire',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Tungabhadra',\n",
       " 'region',\n",
       " 'of',\n",
       " 'modern',\n",
       " 'central',\n",
       " 'Karnataka',\n",
       " ',',\n",
       " 'India',\n",
       " ',',\n",
       " 'during']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize    # rezex base hota hai \n",
    "\n",
    "# it split the word and punntctation sepratley \n",
    "\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c1f1b80-4487-42c1-8dce-335075cbe8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', \"'nt\", 'rahul']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"this is 'nt rahul\")   # samjhne wali sab hai word tokenize me and wordpunct_tokenize me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e5f069-0969-4c3c-899f-9b8f21c643c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', \"'\", 'nt', 'rahul']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(\"this is 'nt rahul\") # wordpunct_tokenizer comma tak ko seprate kr deta hai but word_tokenizer nhi kr pata "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6588be9-324b-4eab-a7fb-4e0af8b302a4",
   "metadata": {},
   "source": [
    "# difference between word_tokenize and wordpunct_tokenize \n",
    "\n",
    "## word_tokenize >> this is pretrain and does'nt consider puctation(single quote,double quote )\n",
    "\n",
    "## wordpunct_tokenize >> this is regex base and it consider punctuation \n",
    "\n",
    "##  TreebankWordDetokenizer >> this is rule based using pen treebank corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbfa23f5-e342-4bcf-8502-7c8a704d0bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t h i s   i s   r a h u l   k u m a r'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordDetokenizer # rule based using the rules penn treebank corpus\n",
    "\n",
    "token=TreebankWordDetokenizer()\n",
    "token.tokenize(\"this is rahul kumar\")  # ye ak ak character ko tokenize kr diya hai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b830df7-0d93-45b2-a5bc-9a33331e827d",
   "metadata": {},
   "source": [
    "# using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "511aae28-b440-412f-b80d-6f94e4cbde0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.19.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.3.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (74.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04397649-f801-4608-820b-d1b76c2e3456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 2.1/12.8 MB 13.0 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.7/12.8 MB 14.2 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 10.8 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 6.0/12.8 MB 8.6 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.3/12.8 MB 7.1 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.6/12.8 MB 5.4 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 4.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 4.0 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 3.5 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/12.8 MB 3.2 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 3.2 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.2/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.0 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dd6ed19-673d-4396-a2b1-925259181f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm   # direct import\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d3558ab-0b90-45a2-9491-2f1fa65f24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=nlp(\"ram is a good boy. he is playing is'nt cricket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eed8788-2279-43c5-9ff8-b051f4d3b24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ram',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'boy',\n",
       " '.',\n",
       " 'he',\n",
       " 'is',\n",
       " 'playing',\n",
       " \"is'nt\",\n",
       " 'cricket']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.text for i in nn]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c01d882-86cd-47ef-bc67-8bc4f16473c9",
   "metadata": {},
   "source": [
    "# preprocessing some start krte hai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe7324e0-a215-45f0-86a3-8bd842d63c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shyam,'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=\"shyam,\"\n",
    "A             # name me comma aa gya esko kaise remove kru "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bfcd830-f642-4502-b827-5ee509b4831e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shyam'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# phle step >> slicing kr ke kr sakte hai \n",
    "\n",
    "A[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c95a5853-b500-48ac-a38d-9284ce07126f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pwskill']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=\" pwskill \"\n",
    "#space remove krna hai kaise krenge \n",
    "a.split()  # ho gya space remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "578a1e02-0b40-4e06-a94a-d88692c457c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' PWSKILL '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5539aba-9f8f-4732-8411-4067aad4d666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' pwskill '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f630688e-44da-4bbb-9e46-8425d00863b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"I have 10 choclate\"\n",
    "\n",
    "# you can do thing with number above \n",
    "# first >> remove 10\n",
    "## second >> convert 10 into text(ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db4b72-d77b-46a3-89b3-d5896d76d33f",
   "metadata": {},
   "source": [
    "# solution >> Regex means regular expression\n",
    "\n",
    "## it is a way to define a pattern for searching or manipulating strings \n",
    "## it is used to match ,search ,replace and manipulate textual data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7baaa20-73ff-46ae-8657-17f1c9a42a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using re module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fe6b515-030b-4478-885b-0ebe817efce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "184794d9-0ff8-42ec-be84-3f57cf62656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package re:\n",
      "\n",
      "NAME\n",
      "    re - Support for regular expressions (RE).\n",
      "\n",
      "MODULE REFERENCE\n",
      "    https://docs.python.org/3.12/library/re.html\n",
      "\n",
      "    The following documentation is automatically generated from the Python\n",
      "    source files.  It may be incomplete, incorrect or include features that\n",
      "    are considered implementation detail and may vary between Python\n",
      "    implementations.  When in doubt, consult the module reference at the\n",
      "    location listed above.\n",
      "\n",
      "DESCRIPTION\n",
      "    This module provides regular expression matching operations similar to\n",
      "    those found in Perl.  It supports both 8-bit and Unicode strings; both\n",
      "    the pattern and the strings being processed can contain null bytes and\n",
      "    characters outside the US ASCII range.\n",
      "\n",
      "    Regular expressions can contain both special and ordinary characters.\n",
      "    Most ordinary characters, like \"A\", \"a\", or \"0\", are the simplest\n",
      "    regular expressions; they simply match themselves.  You can\n",
      "    concatenate ordinary characters, so last matches the string 'last'.\n",
      "\n",
      "    The special characters are:\n",
      "        \".\"      Matches any character except a newline.\n",
      "        \"^\"      Matches the start of the string.\n",
      "        \"$\"      Matches the end of the string or just before the newline at\n",
      "                 the end of the string.\n",
      "        \"*\"      Matches 0 or more (greedy) repetitions of the preceding RE.\n",
      "                 Greedy means that it will match as many repetitions as possible.\n",
      "        \"+\"      Matches 1 or more (greedy) repetitions of the preceding RE.\n",
      "        \"?\"      Matches 0 or 1 (greedy) of the preceding RE.\n",
      "        *?,+?,?? Non-greedy versions of the previous three special characters.\n",
      "        {m,n}    Matches from m to n repetitions of the preceding RE.\n",
      "        {m,n}?   Non-greedy version of the above.\n",
      "        \"\\\\\"     Either escapes special characters or signals a special sequence.\n",
      "        []       Indicates a set of characters.\n",
      "                 A \"^\" as the first character indicates a complementing set.\n",
      "        \"|\"      A|B, creates an RE that will match either A or B.\n",
      "        (...)    Matches the RE inside the parentheses.\n",
      "                 The contents can be retrieved or matched later in the string.\n",
      "        (?aiLmsux) The letters set the corresponding flags defined below.\n",
      "        (?:...)  Non-grouping version of regular parentheses.\n",
      "        (?P<name>...) The substring matched by the group is accessible by name.\n",
      "        (?P=name)     Matches the text matched earlier by the group named name.\n",
      "        (?#...)  A comment; ignored.\n",
      "        (?=...)  Matches if ... matches next, but doesn't consume the string.\n",
      "        (?!...)  Matches if ... doesn't match next.\n",
      "        (?<=...) Matches if preceded by ... (must be fixed length).\n",
      "        (?<!...) Matches if not preceded by ... (must be fixed length).\n",
      "        (?(id/name)yes|no) Matches yes pattern if the group with id/name matched,\n",
      "                           the (optional) no pattern otherwise.\n",
      "\n",
      "    The special sequences consist of \"\\\\\" and a character from the list\n",
      "    below.  If the ordinary character is not on the list, then the\n",
      "    resulting RE will match the second character.\n",
      "        \\number  Matches the contents of the group of the same number.\n",
      "        \\A       Matches only at the start of the string.\n",
      "        \\Z       Matches only at the end of the string.\n",
      "        \\b       Matches the empty string, but only at the start or end of a word.\n",
      "        \\B       Matches the empty string, but not at the start or end of a word.\n",
      "        \\d       Matches any decimal digit; equivalent to the set [0-9] in\n",
      "                 bytes patterns or string patterns with the ASCII flag.\n",
      "                 In string patterns without the ASCII flag, it will match the whole\n",
      "                 range of Unicode digits.\n",
      "        \\D       Matches any non-digit character; equivalent to [^\\d].\n",
      "        \\s       Matches any whitespace character; equivalent to [ \\t\\n\\r\\f\\v] in\n",
      "                 bytes patterns or string patterns with the ASCII flag.\n",
      "                 In string patterns without the ASCII flag, it will match the whole\n",
      "                 range of Unicode whitespace characters.\n",
      "        \\S       Matches any non-whitespace character; equivalent to [^\\s].\n",
      "        \\w       Matches any alphanumeric character; equivalent to [a-zA-Z0-9_]\n",
      "                 in bytes patterns or string patterns with the ASCII flag.\n",
      "                 In string patterns without the ASCII flag, it will match the\n",
      "                 range of Unicode alphanumeric characters (letters plus digits\n",
      "                 plus underscore).\n",
      "                 With LOCALE, it will match the set [0-9_] plus characters defined\n",
      "                 as letters for the current locale.\n",
      "        \\W       Matches the complement of \\w.\n",
      "        \\\\       Matches a literal backslash.\n",
      "\n",
      "    This module exports the following functions:\n",
      "        match     Match a regular expression pattern to the beginning of a string.\n",
      "        fullmatch Match a regular expression pattern to all of a string.\n",
      "        search    Search a string for the presence of a pattern.\n",
      "        sub       Substitute occurrences of a pattern found in a string.\n",
      "        subn      Same as sub, but also return the number of substitutions made.\n",
      "        split     Split a string by the occurrences of a pattern.\n",
      "        findall   Find all occurrences of a pattern in a string.\n",
      "        finditer  Return an iterator yielding a Match object for each match.\n",
      "        compile   Compile a pattern into a Pattern object.\n",
      "        purge     Clear the regular expression cache.\n",
      "        escape    Backslash all non-alphanumerics in a string.\n",
      "\n",
      "    Each function other than purge and escape can take an optional 'flags' argument\n",
      "    consisting of one or more of the following module constants, joined by \"|\".\n",
      "    A, L, and U are mutually exclusive.\n",
      "        A  ASCII       For string patterns, make \\w, \\W, \\b, \\B, \\d, \\D\n",
      "                       match the corresponding ASCII character categories\n",
      "                       (rather than the whole Unicode categories, which is the\n",
      "                       default).\n",
      "                       For bytes patterns, this flag is the only available\n",
      "                       behaviour and needn't be specified.\n",
      "        I  IGNORECASE  Perform case-insensitive matching.\n",
      "        L  LOCALE      Make \\w, \\W, \\b, \\B, dependent on the current locale.\n",
      "        M  MULTILINE   \"^\" matches the beginning of lines (after a newline)\n",
      "                       as well as the string.\n",
      "                       \"$\" matches the end of lines (before a newline) as well\n",
      "                       as the end of the string.\n",
      "        S  DOTALL      \".\" matches any character at all, including the newline.\n",
      "        X  VERBOSE     Ignore whitespace and comments for nicer looking RE's.\n",
      "        U  UNICODE     For compatibility only. Ignored for string patterns (it\n",
      "                       is the default), and forbidden for bytes patterns.\n",
      "\n",
      "    This module also defines an exception 'error'.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _casefix\n",
      "    _compiler\n",
      "    _constants\n",
      "    _parser\n",
      "\n",
      "CLASSES\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        error\n",
      "    builtins.object\n",
      "        Match\n",
      "        Pattern\n",
      "    enum.IntFlag(builtins.int, enum.ReprEnum, enum.Flag)\n",
      "        RegexFlag\n",
      "\n",
      "    class Match(builtins.object)\n",
      "     |  The result of re.match() and re.search().\n",
      "     |  Match objects always have a boolean value of True.\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __copy__(self, /)\n",
      "     |\n",
      "     |  __deepcopy__(self, memo, /)\n",
      "     |\n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |\n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  end(self, group=0, /)\n",
      "     |      Return index of the end of the substring matched by group.\n",
      "     |\n",
      "     |  expand(self, /, template)\n",
      "     |      Return the string obtained by doing backslash substitution on the string template, as done by the sub() method.\n",
      "     |\n",
      "     |  group(...)\n",
      "     |      group([group1, ...]) -> str or tuple.\n",
      "     |      Return subgroup(s) of the match by indices or names.\n",
      "     |      For 0 returns the entire match.\n",
      "     |\n",
      "     |  groupdict(self, /, default=None)\n",
      "     |      Return a dictionary containing all the named subgroups of the match, keyed by the subgroup name.\n",
      "     |\n",
      "     |      default\n",
      "     |        Is used for groups that did not participate in the match.\n",
      "     |\n",
      "     |  groups(self, /, default=None)\n",
      "     |      Return a tuple containing all the subgroups of the match, from 1.\n",
      "     |\n",
      "     |      default\n",
      "     |        Is used for groups that did not participate in the match.\n",
      "     |\n",
      "     |  span(self, group=0, /)\n",
      "     |      For match object m, return the 2-tuple (m.start(group), m.end(group)).\n",
      "     |\n",
      "     |  start(self, group=0, /)\n",
      "     |      Return index of the start of the substring matched by group.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |\n",
      "     |  __class_getitem__(...)\n",
      "     |      See PEP 585\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  endpos\n",
      "     |      The index into the string beyond which the RE engine will not go.\n",
      "     |\n",
      "     |  lastgroup\n",
      "     |      The name of the last matched capturing group.\n",
      "     |\n",
      "     |  lastindex\n",
      "     |      The integer index of the last matched capturing group.\n",
      "     |\n",
      "     |  pos\n",
      "     |      The index into the string at which the RE engine started looking for a match.\n",
      "     |\n",
      "     |  re\n",
      "     |      The regular expression object.\n",
      "     |\n",
      "     |  regs\n",
      "     |\n",
      "     |  string\n",
      "     |      The string passed to match() or search().\n",
      "\n",
      "    class Pattern(builtins.object)\n",
      "     |  Compiled regular expression object.\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __copy__(self, /)\n",
      "     |\n",
      "     |  __deepcopy__(self, memo, /)\n",
      "     |\n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |\n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |\n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |\n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |\n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |\n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |\n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  findall(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Return a list of all non-overlapping matches of pattern in string.\n",
      "     |\n",
      "     |  finditer(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Return an iterator over all non-overlapping matches for the RE pattern in string.\n",
      "     |\n",
      "     |      For each match, the iterator returns a match object.\n",
      "     |\n",
      "     |  fullmatch(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Matches against all of the string.\n",
      "     |\n",
      "     |  match(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Matches zero or more characters at the beginning of the string.\n",
      "     |\n",
      "     |  scanner(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |\n",
      "     |  search(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Scan through string looking for a match, and return a corresponding match object instance.\n",
      "     |\n",
      "     |      Return None if no position in the string matches.\n",
      "     |\n",
      "     |  split(self, /, string, maxsplit=0)\n",
      "     |      Split string by the occurrences of pattern.\n",
      "     |\n",
      "     |  sub(self, /, repl, string, count=0)\n",
      "     |      Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl.\n",
      "     |\n",
      "     |  subn(self, /, repl, string, count=0)\n",
      "     |      Return the tuple (new_string, number_of_subs_made) found by replacing the leftmost non-overlapping occurrences of pattern with the replacement repl.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |\n",
      "     |  __class_getitem__(...)\n",
      "     |      See PEP 585\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  flags\n",
      "     |      The regex matching flags.\n",
      "     |\n",
      "     |  groupindex\n",
      "     |      A dictionary mapping group names to group numbers.\n",
      "     |\n",
      "     |  groups\n",
      "     |      The number of capturing groups in the pattern.\n",
      "     |\n",
      "     |  pattern\n",
      "     |      The pattern string from which the RE object was compiled.\n",
      "\n",
      "    class RegexFlag(enum.IntFlag)\n",
      "     |  RegexFlag(*values)\n",
      "     |\n",
      "     |  An enumeration.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      RegexFlag\n",
      "     |      enum.IntFlag\n",
      "     |      builtins.int\n",
      "     |      enum.ReprEnum\n",
      "     |      enum.Flag\n",
      "     |      enum.Enum\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __and__(self, other) from enum.Flag\n",
      "     |\n",
      "     |  __format__(self, format_spec, /) from builtins.int\n",
      "     |      Convert to a string according to format_spec.\n",
      "     |\n",
      "     |  __invert__(self) from enum.Flag\n",
      "     |\n",
      "     |  __new__(cls, value) from enum.Enum\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |\n",
      "     |  __or__(self, other) from enum.Flag\n",
      "     |      Return self|value.\n",
      "     |\n",
      "     |  __rand__ = __and__(self, other)\n",
      "     |\n",
      "     |  __repr__ = global_flag_repr(self) from enum\n",
      "     |      use module.flag_name instead of class.flag_name\n",
      "     |\n",
      "     |      the module is the last module in case of a multi-module name\n",
      "     |\n",
      "     |  __ror__ = __or__(self, other)\n",
      "     |\n",
      "     |  __rxor__ = __xor__(self, other)\n",
      "     |\n",
      "     |  __str__(self, /) from builtins.object\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  __xor__(self, other) from enum.Flag\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  ASCII = re.ASCII\n",
      "     |\n",
      "     |  DEBUG = re.DEBUG\n",
      "     |\n",
      "     |  DOTALL = re.DOTALL\n",
      "     |\n",
      "     |  IGNORECASE = re.IGNORECASE\n",
      "     |\n",
      "     |  LOCALE = re.LOCALE\n",
      "     |\n",
      "     |  MULTILINE = re.MULTILINE\n",
      "     |\n",
      "     |  TEMPLATE = re.TEMPLATE\n",
      "     |\n",
      "     |  UNICODE = re.UNICODE\n",
      "     |\n",
      "     |  VERBOSE = re.VERBOSE\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.int:\n",
      "     |\n",
      "     |  __abs__(self, /)\n",
      "     |      abs(self)\n",
      "     |\n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |\n",
      "     |  __bool__(self, /)\n",
      "     |      True if self else False\n",
      "     |\n",
      "     |  __ceil__(...)\n",
      "     |      Ceiling of an Integral returns itself.\n",
      "     |\n",
      "     |  __divmod__(self, value, /)\n",
      "     |      Return divmod(self, value).\n",
      "     |\n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __float__(self, /)\n",
      "     |      float(self)\n",
      "     |\n",
      "     |  __floor__(...)\n",
      "     |      Flooring an Integral returns itself.\n",
      "     |\n",
      "     |  __floordiv__(self, value, /)\n",
      "     |      Return self//value.\n",
      "     |\n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |\n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |\n",
      "     |  __getnewargs__(self, /)\n",
      "     |\n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |\n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |\n",
      "     |  __index__(self, /)\n",
      "     |      Return self converted to an integer, if self is suitable for use as an index into a list.\n",
      "     |\n",
      "     |  __int__(self, /)\n",
      "     |      int(self)\n",
      "     |\n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |\n",
      "     |  __lshift__(self, value, /)\n",
      "     |      Return self<<value.\n",
      "     |\n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |\n",
      "     |  __mod__(self, value, /)\n",
      "     |      Return self%value.\n",
      "     |\n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |\n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |\n",
      "     |  __neg__(self, /)\n",
      "     |      -self\n",
      "     |\n",
      "     |  __pos__(self, /)\n",
      "     |      +self\n",
      "     |\n",
      "     |  __pow__(self, value, mod=None, /)\n",
      "     |      Return pow(self, value, mod).\n",
      "     |\n",
      "     |  __radd__(self, value, /)\n",
      "     |      Return value+self.\n",
      "     |\n",
      "     |  __rdivmod__(self, value, /)\n",
      "     |      Return divmod(value, self).\n",
      "     |\n",
      "     |  __rfloordiv__(self, value, /)\n",
      "     |      Return value//self.\n",
      "     |\n",
      "     |  __rlshift__(self, value, /)\n",
      "     |      Return value<<self.\n",
      "     |\n",
      "     |  __rmod__(self, value, /)\n",
      "     |      Return value%self.\n",
      "     |\n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |\n",
      "     |  __round__(...)\n",
      "     |      Rounding an Integral returns itself.\n",
      "     |\n",
      "     |      Rounding with an ndigits argument also returns an integer.\n",
      "     |\n",
      "     |  __rpow__(self, value, mod=None, /)\n",
      "     |      Return pow(value, self, mod).\n",
      "     |\n",
      "     |  __rrshift__(self, value, /)\n",
      "     |      Return value>>self.\n",
      "     |\n",
      "     |  __rshift__(self, value, /)\n",
      "     |      Return self>>value.\n",
      "     |\n",
      "     |  __rsub__(self, value, /)\n",
      "     |      Return value-self.\n",
      "     |\n",
      "     |  __rtruediv__(self, value, /)\n",
      "     |      Return value/self.\n",
      "     |\n",
      "     |  __sizeof__(self, /)\n",
      "     |      Returns size in memory, in bytes.\n",
      "     |\n",
      "     |  __sub__(self, value, /)\n",
      "     |      Return self-value.\n",
      "     |\n",
      "     |  __truediv__(self, value, /)\n",
      "     |      Return self/value.\n",
      "     |\n",
      "     |  __trunc__(...)\n",
      "     |      Truncating an Integral returns itself.\n",
      "     |\n",
      "     |  as_integer_ratio(self, /)\n",
      "     |      Return a pair of integers, whose ratio is equal to the original int.\n",
      "     |\n",
      "     |      The ratio is in lowest terms and has a positive denominator.\n",
      "     |\n",
      "     |      >>> (10).as_integer_ratio()\n",
      "     |      (10, 1)\n",
      "     |      >>> (-10).as_integer_ratio()\n",
      "     |      (-10, 1)\n",
      "     |      >>> (0).as_integer_ratio()\n",
      "     |      (0, 1)\n",
      "     |\n",
      "     |  bit_count(self, /)\n",
      "     |      Number of ones in the binary representation of the absolute value of self.\n",
      "     |\n",
      "     |      Also known as the population count.\n",
      "     |\n",
      "     |      >>> bin(13)\n",
      "     |      '0b1101'\n",
      "     |      >>> (13).bit_count()\n",
      "     |      3\n",
      "     |\n",
      "     |  bit_length(self, /)\n",
      "     |      Number of bits necessary to represent self in binary.\n",
      "     |\n",
      "     |      >>> bin(37)\n",
      "     |      '0b100101'\n",
      "     |      >>> (37).bit_length()\n",
      "     |      6\n",
      "     |\n",
      "     |  conjugate(...)\n",
      "     |      Returns self, the complex conjugate of any int.\n",
      "     |\n",
      "     |  is_integer(self, /)\n",
      "     |      Returns True. Exists for duck type compatibility with float.is_integer.\n",
      "     |\n",
      "     |  to_bytes(self, /, length=1, byteorder='big', *, signed=False)\n",
      "     |      Return an array of bytes representing an integer.\n",
      "     |\n",
      "     |      length\n",
      "     |        Length of bytes object to use.  An OverflowError is raised if the\n",
      "     |        integer is not representable with the given number of bytes.  Default\n",
      "     |        is length 1.\n",
      "     |      byteorder\n",
      "     |        The byte order used to represent the integer.  If byteorder is 'big',\n",
      "     |        the most significant byte is at the beginning of the byte array.  If\n",
      "     |        byteorder is 'little', the most significant byte is at the end of the\n",
      "     |        byte array.  To request the native byte order of the host system, use\n",
      "     |        `sys.byteorder' as the byte order value.  Default is to use 'big'.\n",
      "     |      signed\n",
      "     |        Determines whether two's complement is used to represent the integer.\n",
      "     |        If signed is False and a negative integer is given, an OverflowError\n",
      "     |        is raised.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.int:\n",
      "     |\n",
      "     |  from_bytes(bytes, byteorder='big', *, signed=False)\n",
      "     |      Return the integer represented by the given array of bytes.\n",
      "     |\n",
      "     |      bytes\n",
      "     |        Holds the array of bytes to convert.  The argument must either\n",
      "     |        support the buffer protocol or be an iterable object producing bytes.\n",
      "     |        Bytes and bytearray are examples of built-in objects that support the\n",
      "     |        buffer protocol.\n",
      "     |      byteorder\n",
      "     |        The byte order used to represent the integer.  If byteorder is 'big',\n",
      "     |        the most significant byte is at the beginning of the byte array.  If\n",
      "     |        byteorder is 'little', the most significant byte is at the end of the\n",
      "     |        byte array.  To request the native byte order of the host system, use\n",
      "     |        `sys.byteorder' as the byte order value.  Default is to use 'big'.\n",
      "     |      signed\n",
      "     |        Indicates whether two's complement is used to represent the integer.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.int:\n",
      "     |\n",
      "     |  denominator\n",
      "     |      the denominator of a rational number in lowest terms\n",
      "     |\n",
      "     |  imag\n",
      "     |      the imaginary part of a complex number\n",
      "     |\n",
      "     |  numerator\n",
      "     |      the numerator of a rational number in lowest terms\n",
      "     |\n",
      "     |  real\n",
      "     |      the real part of a complex number\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from enum.Flag:\n",
      "     |\n",
      "     |  __contains__(self, other)\n",
      "     |      Returns True if self has at least the same flags set as other.\n",
      "     |\n",
      "     |  __iter__(self)\n",
      "     |      Returns flags in definition order.\n",
      "     |\n",
      "     |  __len__(self)\n",
      "     |      Return the number of members (no aliases)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from enum.Enum:\n",
      "     |\n",
      "     |  __dir__(self)\n",
      "     |      Returns public methods and other interesting attributes.\n",
      "     |\n",
      "     |  __init__(self, *args, **kwds)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  __reduce_ex__(self, proto)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from enum.Enum:\n",
      "     |\n",
      "     |  name\n",
      "     |      The name of the Enum member.\n",
      "     |\n",
      "     |  value\n",
      "     |      The value of the Enum member.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from enum.EnumType:\n",
      "     |\n",
      "     |  __getitem__(name)\n",
      "     |      Return the member matching `name`.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from enum.EnumType:\n",
      "     |\n",
      "     |  __members__\n",
      "     |      Returns a mapping of member name->value.\n",
      "     |\n",
      "     |      This mapping lists all enum members, including aliases. Note that this\n",
      "     |      is a read-only view of the internal mapping.\n",
      "\n",
      "    class error(builtins.Exception)\n",
      "     |  error(msg, pattern=None, pos=None)\n",
      "     |\n",
      "     |  Exception raised for invalid regular expressions.\n",
      "     |\n",
      "     |  Attributes:\n",
      "     |\n",
      "     |      msg: The unformatted error message\n",
      "     |      pattern: The regular expression pattern\n",
      "     |      pos: The index in the pattern where compilation failed (may be None)\n",
      "     |      lineno: The line corresponding to pos (may be None)\n",
      "     |      colno: The column corresponding to pos (may be None)\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      error\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, msg, pattern=None, pos=None) from re._constants.error\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |\n",
      "     |  __new__(*args, **kwargs) class method of builtins.Exception\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |\n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |\n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setstate__(...)\n",
      "     |\n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  add_note(...)\n",
      "     |      Exception.add_note(note) --\n",
      "     |      add a note to the exception\n",
      "     |\n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |\n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |\n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |\n",
      "     |  __dict__\n",
      "     |\n",
      "     |  __suppress_context__\n",
      "     |\n",
      "     |  __traceback__\n",
      "     |\n",
      "     |  args\n",
      "\n",
      "FUNCTIONS\n",
      "    compile(pattern, flags=0)\n",
      "        Compile a regular expression pattern, returning a Pattern object.\n",
      "\n",
      "    escape(pattern)\n",
      "        Escape special characters in a string.\n",
      "\n",
      "    findall(pattern, string, flags=0)\n",
      "        Return a list of all non-overlapping matches in the string.\n",
      "\n",
      "        If one or more capturing groups are present in the pattern, return\n",
      "        a list of groups; this will be a list of tuples if the pattern\n",
      "        has more than one group.\n",
      "\n",
      "        Empty matches are included in the result.\n",
      "\n",
      "    finditer(pattern, string, flags=0)\n",
      "        Return an iterator over all non-overlapping matches in the\n",
      "        string.  For each match, the iterator returns a Match object.\n",
      "\n",
      "        Empty matches are included in the result.\n",
      "\n",
      "    fullmatch(pattern, string, flags=0)\n",
      "        Try to apply the pattern to all of the string, returning\n",
      "        a Match object, or None if no match was found.\n",
      "\n",
      "    match(pattern, string, flags=0)\n",
      "        Try to apply the pattern at the start of the string, returning\n",
      "        a Match object, or None if no match was found.\n",
      "\n",
      "    purge()\n",
      "        Clear the regular expression caches\n",
      "\n",
      "    search(pattern, string, flags=0)\n",
      "        Scan through string looking for a match to the pattern, returning\n",
      "        a Match object, or None if no match was found.\n",
      "\n",
      "    split(pattern, string, maxsplit=0, flags=0)\n",
      "        Split the source string by the occurrences of the pattern,\n",
      "        returning a list containing the resulting substrings.  If\n",
      "        capturing parentheses are used in pattern, then the text of all\n",
      "        groups in the pattern are also returned as part of the resulting\n",
      "        list.  If maxsplit is nonzero, at most maxsplit splits occur,\n",
      "        and the remainder of the string is returned as the final element\n",
      "        of the list.\n",
      "\n",
      "    sub(pattern, repl, string, count=0, flags=0)\n",
      "        Return the string obtained by replacing the leftmost\n",
      "        non-overlapping occurrences of the pattern in string by the\n",
      "        replacement repl.  repl can be either a string or a callable;\n",
      "        if a string, backslash escapes in it are processed.  If it is\n",
      "        a callable, it's passed the Match object and must return\n",
      "        a replacement string to be used.\n",
      "\n",
      "    subn(pattern, repl, string, count=0, flags=0)\n",
      "        Return a 2-tuple containing (new_string, number).\n",
      "        new_string is the string obtained by replacing the leftmost\n",
      "        non-overlapping occurrences of the pattern in the source\n",
      "        string by the replacement repl.  number is the number of\n",
      "        substitutions that were made. repl can be either a string or a\n",
      "        callable; if a string, backslash escapes in it are processed.\n",
      "        If it is a callable, it's passed the Match object and must\n",
      "        return a replacement string to be used.\n",
      "\n",
      "    template(pattern, flags=0)\n",
      "        Compile a template pattern, returning a Pattern object, deprecated\n",
      "\n",
      "DATA\n",
      "    A = re.ASCII\n",
      "    ASCII = re.ASCII\n",
      "    DOTALL = re.DOTALL\n",
      "    I = re.IGNORECASE\n",
      "    IGNORECASE = re.IGNORECASE\n",
      "    L = re.LOCALE\n",
      "    LOCALE = re.LOCALE\n",
      "    M = re.MULTILINE\n",
      "    MULTILINE = re.MULTILINE\n",
      "    NOFLAG = re.NOFLAG\n",
      "    S = re.DOTALL\n",
      "    U = re.UNICODE\n",
      "    UNICODE = re.UNICODE\n",
      "    VERBOSE = re.VERBOSE\n",
      "    X = re.VERBOSE\n",
      "    __all__ = ['match', 'fullmatch', 'search', 'sub', 'subn', 'split', 'fi...\n",
      "\n",
      "VERSION\n",
      "    2.2.1\n",
      "\n",
      "FILE\n",
      "    c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\re\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a3ebb8d-b595-40b4-9240-a3837d0249ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10', '1']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1 >> write a regular expression to search / find all the digit inside a string \n",
    "\n",
    "\n",
    "data=\"I have 10 choclate and 1 bat \"\n",
    "\n",
    "re.findall(r\"\\d+\",data) # r means regular \"\\d\" means jaha bhi digit mile o do [\\d =digit]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91c3ffd9-b4f8-46e8-a5e8-bb23165068b1",
   "metadata": {},
   "source": [
    "12345678901           # \\d*                digits (0    )\n",
    "$12345678901          # \\$\\d*             $    digits\n",
    "$12345678901.42       # \\$\\d*\\.\\d+        $ + digits + dot[character] +    1 digit (decimal number)\n",
    "$12345678901.42       # \\$\\d*\\.\\d{2}      $ + digits + dot[character] +  2 digits (currency format  $123.45)\n",
    "123-456-7890          # \\d{3}-\\d{3}-\\d{4}   format XXX-XXX-XXXX\n",
    "Austrialia            # [A-Z]\\w*          Capital letter     word ( country name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95cad040-b66c-4563-87c8-a72678534f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '', '', '', '12310', '']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example \n",
    "\n",
    "\n",
    "sentence=\"this is otp 12310\"\n",
    "re.findall(r\"\\d*\",sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0b135cb-9863-41a3-aa73-cee96a52e2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12310']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\d+\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9d81059-1692-431a-9efa-3fbd740b59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"my salary is $200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b17ce39-1235-4ee3-b1e0-c91e7191c062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$200']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\$\\d*\",s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe6cb684-481d-4721-a2f7-f3fe7234dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"my salary is #200\"   # $ SING KE JAGHA KUCH BHI LIKH SAKTE HAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbe5148c-127e-4481-9e87-414e449dedde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#200']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\#\\d*\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46dad190-b3df-4f6b-a7d6-65c1318b355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$200.55']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=\"my payment is $200.55 is successful\"\n",
    "re.findall(r\"\\$\\d*\\.\\d+\",p)\n",
    "re.findall(r\"\\$\\d*\\.\\d*\",p)   # dono chalega "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dab6c41e-895e-4a94-bdf4-b5b4c038cbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$200.55', '$55.2']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=\"my payment is $200.55 is $55.2 successful\"\n",
    "re.findall(r\"\\$\\d*\\.\\d+\",p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25207f3f-c27b-4b4b-b6fa-d32a7e2cb5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$200.55', '55.2']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=\"my payment is $200.55 is 55.2 successful\"\n",
    "re.findall(r\"[\\$\\]\\d*\\.\\d+\",p)  #  control+shift+4   se rupee sign aa jata hai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "577bf218-b14c-4bba-8ca0-9241d755dca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$200.55', '$55.2']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=\"my payment is $200.55 is $55.2 successful\"\n",
    "re.findall(r\"[\\$]\\d+(?:\\.\\d+)\",p)  # (?:\\.\\d+) this will match decimal part "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c857a-0da3-4f08-bdec-b1a712f39041",
   "metadata": {},
   "source": [
    "# Refer to regex notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6796c5c-7fc6-4934-8217-425679f39c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have  choclate and  candle'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the number \n",
    "\n",
    "t=\"i have 6 choclate and 3 candle\"\n",
    "re.sub(r\"\\d+\",\"\",t)  # dekh sakte hai ki 6 ko replace kr diye "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c1f9e7-9d92-4163-a0d6-d5ddaec93468",
   "metadata": {},
   "source": [
    "# nya chij krte haai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c889313-fb22-49dd-85db-4b9fb2c721e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inflect\n",
      "  Downloading inflect-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting more_itertools>=8.5.0 (from inflect)\n",
      "  Downloading more_itertools-10.8.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from inflect) (4.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rk318\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typeguard>=4.0.1->inflect) (4.12.2)\n",
      "Downloading inflect-7.5.0-py3-none-any.whl (35 kB)\n",
      "Downloading more_itertools-10.8.0-py3-none-any.whl (69 kB)\n",
      "Installing collected packages: more_itertools, inflect\n",
      "Successfully installed inflect-7.5.0 more_itertools-10.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c510e607-e3ed-4a5d-8a84-0d1b0009e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "q=inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b34aef8-102a-4d20-8488-99d70fd67fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'six'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.number_to_words(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "20107a17-8558-4f6c-8328-d7818554307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'three', 'car', 'and', 'seven', 'baffelo']\n"
     ]
    }
   ],
   "source": [
    "input_str=\"i have 3 car and 7 baffelo\"\n",
    "cleaned_sent=[]\n",
    "for i in input_str.split():\n",
    "    if i.isdigit():\n",
    "        temp=q.number_to_words(i)\n",
    "        cleaned_sent.append(temp)\n",
    "    else:\n",
    "        cleaned_sent.append(i)\n",
    "print(cleaned_sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab107f-b871-47de-99b8-81eced0013b6",
   "metadata": {},
   "source": [
    " # Pancuation remove  krte ghai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d78c2ede-1199-49b4-9636-f1b3cef3bbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation  # jitna bhi punctuation hoota hai sab aa gya  ab esi ko removekrne  hai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7c20bcc3-b8da-471c-af85-05184d5ab923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ak function banate hai \n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # creating translation \n",
    "    translater=str.maketrans(\"\",\"\",string.punctuation)\n",
    "    return text.translate(translater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e97898d9-3ede-4bbc-8426-c80d7362254d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heyare you exitedafter a week we will be in shimla '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation(\"hey,are you exited??,after a week ,we will be in shimla !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "adf86b3e-e3d5-4b13-8aea-2294c4a7fd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function str.translate(table, /)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"hey,are you exited??,after a week ,we will be in shimla !!\"\n",
    "text.translate  # means >> The translate() method returns a string where some specified characters \n",
    "                # are replaced with the character described in a dictionary, or in a mapping table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e80d5b2-938d-4d6a-aa15-b80339050445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello Pir\n"
     ]
    }
   ],
   "source": [
    "txt=\"hello sir\"\n",
    "pp=str.maketrans(\"s\",\"P\")\n",
    "print(txt.translate(pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ad6f1de9-ce9d-495d-81c9-301a21f7e3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{115: 80}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp  # ascii value print hoga "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bfd2b3a1-52ed-47de-9529-037309c887a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Advanced Technology: Our text generator is based on the most intelligent, smart, and advanced\\nalgorithm that formulates unique and unfathomably clear, structured, and meaningful sentences that you\\ncan possibly imagine. Its high-tech artificial intelligence allows it to generate texts words that are \\noriginal and human-like, and subtle.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=\"\"\"Advanced Technology: Our text generator is based on the most intelligent, smart, and advanced\n",
    "algorithm that formulates unique and unfathomably clear, structured, and meaningful sentences that you\n",
    "can possibly imagine. Its high-tech artificial intelligence allows it to generate texts words that are \n",
    "original and human-like, and subtle.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "288e1aae-fbb5-497d-8207-fb31adb239d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Advanced',\n",
       " 'Technology:',\n",
       " 'Our',\n",
       " 'text',\n",
       " 'generator',\n",
       " 'is',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'most',\n",
       " 'intelligent,',\n",
       " 'smart,',\n",
       " 'and',\n",
       " 'advanced',\n",
       " 'algorithm',\n",
       " 'that',\n",
       " 'formulates',\n",
       " 'unique',\n",
       " 'and',\n",
       " 'unfathomably',\n",
       " 'clear,',\n",
       " 'structured,',\n",
       " 'and',\n",
       " 'meaningful',\n",
       " 'sentences',\n",
       " 'that',\n",
       " 'you',\n",
       " 'can',\n",
       " 'possibly',\n",
       " 'imagine.',\n",
       " 'Its',\n",
       " 'high-tech',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'allows',\n",
       " 'it',\n",
       " 'to',\n",
       " 'generate',\n",
       " 'texts',\n",
       " 'words',\n",
       " 'that',\n",
       " 'are',\n",
       " 'original',\n",
       " 'and',\n",
       " 'human-like,',\n",
       " 'and',\n",
       " 'subtle.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize them \n",
    "\n",
    "\n",
    "from nltk.tokenize.regexp import WhitespaceTokenizer  # on space tokenizer \n",
    "tokens=WhitespaceTokenizer().tokenize(m)\n",
    "tokens  # repeting value hai unique value kaise  nikle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c7f96d87-5d4b-45a5-857e-70c215f81ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Advanced',\n",
       " 'Its',\n",
       " 'Our',\n",
       " 'Technology:',\n",
       " 'advanced',\n",
       " 'algorithm',\n",
       " 'allows',\n",
       " 'and',\n",
       " 'are',\n",
       " 'artificial',\n",
       " 'based',\n",
       " 'can',\n",
       " 'clear,',\n",
       " 'formulates',\n",
       " 'generate',\n",
       " 'generator',\n",
       " 'high-tech',\n",
       " 'human-like,',\n",
       " 'imagine.',\n",
       " 'intelligence',\n",
       " 'intelligent,',\n",
       " 'is',\n",
       " 'it',\n",
       " 'meaningful',\n",
       " 'most',\n",
       " 'on',\n",
       " 'original',\n",
       " 'possibly',\n",
       " 'sentences',\n",
       " 'smart,',\n",
       " 'structured,',\n",
       " 'subtle.',\n",
       " 'text',\n",
       " 'texts',\n",
       " 'that',\n",
       " 'the',\n",
       " 'to',\n",
       " 'unfathomably',\n",
       " 'unique',\n",
       " 'words',\n",
       " 'you'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vac=set(tokens)  #unique value nikl gya \n",
    "\n",
    "# vocubalery is unique word in a text \n",
    "vac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540d577-549b-40ac-bfe7-e373a729107a",
   "metadata": {},
   "source": [
    "# Stemming and Lemenitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4e7e9578-c466-41ac-a92d-67d8b2d28c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "ests---->est\n",
      "esten---->esten\n",
      "writing---->write\n",
      "writes---->write\n"
     ]
    }
   ],
   "source": [
    "# Stemming >> process of reducing a word to its word stem(root)[removing prefix and suffix] also called as lema \n",
    "# eating,running,etc >> eat,run temmming hua \n",
    "\n",
    "\n",
    "word=[\"eating\",\"ests\",\"esten\",\"writing\",\"writes\"]\n",
    "\n",
    "# ab esko orignal form me lana hai \n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "s=PorterStemmer() # objects create \n",
    "\n",
    "for i in word:\n",
    "    print(i+\"---->\"+s.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e6ed1538-5cdc-43fd-a68c-2ac3ec21ee85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"running\") # koi bhi word dijiye prefix,siffix nikal kr de deta h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f68c4-877f-4025-8656-d3cdbd99e8d9",
   "metadata": {},
   "source": [
    "# from nltk.stem import PorterStemmer # just remove suffix and prefix with concedring grammar(not getting to the root word >> lema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6d03d4f6-8090-4c9f-90d8-679e5d0481de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unhappi'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"unhappy\")  # grammer thik nhi kr rha hai ya exact root word nhi de rha hai "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b88265f-c1fe-454c-8f9e-c8133c057625",
   "metadata": {},
   "source": [
    "# RegexpStemmer aiya esi ko imporve krne ke liye "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8e04ca07-7a9a-409f-a8ae-b1e0fd479291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "stemmer=RegexpStemmer(\"ing$|s$|e$|able$\",min=4)\n",
    "stemmer.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "da956d83-f098-4b2e-b4f7-1da2bbfe95a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -------> run\n",
      "happily -------> happili\n",
      "books -------> book\n",
      "easily -------> easili\n",
      "studies -------> studi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# language specify   \n",
    "snow = SnowballStemmer(\"english\")\n",
    "\n",
    "#  list of words\n",
    "words = [\"running\", \"happily\", \"books\", \"easily\", \"studies\"]\n",
    "\n",
    "for i in words:\n",
    "    print(i + \" -------> \" + snow.stem(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d899e9-7f6a-4127-9c5c-efe77f8ace4d",
   "metadata": {},
   "source": [
    "# eske liye use krenge lemetization \n",
    "\n",
    "\n",
    "## Lemmatization >> Lemma >> that is root word \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "87e88dcb-0d3f-4c66-9942-e74f01d96558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer  \n",
    "\n",
    "lemetizer=WordNetLemmatizer()\n",
    "lemetizer.lemmatize(\"going\",pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d901ff29-b6ec-4a85-8f8d-aa2208f0da13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rk318\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rk318\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')        # WordNet lemmatizer ke liye\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6d033d82-6cfc-481a-9f1d-6b7cb72b24ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemetizer.lemmatize(\"doing\",pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bcc1f829-bef5-4758-a30c-7bc58e941271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -------> run\n",
      "happily -------> happily\n",
      "books -------> book\n",
      "easily -------> easily\n",
      "studies -------> study\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(i + \" -------> \" + lemetizer.lemmatize(i,pos=\"v\"))  # pos is part of speech is verb "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add68fbe-8284-49a8-9a45-8d0dfbd2ff75",
   "metadata": {},
   "source": [
    "# Stop-word >> means any word jo bahut jaida repeted ho but commonly use hota ho esi ko stop-word bolte hai \n",
    "\n",
    "# stop-word >> stop-word are commonly used word it should be ignore in text analysis\n",
    "\n",
    "## is,this,a, is am ,of etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3db2e02f-bc0c-40b3-b9b2-6b427a27ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rk318\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\") # pura stopword download ho gya "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dbcd4193-c0a6-4769-b6b0-29d562efa2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is an example to demonstrate stop word removing .it is very useful for analysis and to be done after tokenization'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"this is an example to demonstrate stop word removing .it is very useful for analysis and to be done after tokenization\"\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8ebed21c-68e1-46b4-b6e6-0a38ab4bbb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'to',\n",
       " 'demonstrate',\n",
       " 'stop',\n",
       " 'word',\n",
       " 'removing',\n",
       " '.it',\n",
       " 'is',\n",
       " 'very',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'to',\n",
       " 'be',\n",
       " 'done',\n",
       " 'after',\n",
       " 'tokenization']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "word=word_tokenize(text)\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "55fdf09f-64dc-41c7-9567-22d7bc93d008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_w=set(stopwords.words(\"english\")) # ye sab english ke stop word hai \n",
    "stop_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3fcd6629-3a78-4c70-aae9-9eb47dec2e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'to',\n",
       " 'demonstrate',\n",
       " 'stop',\n",
       " 'word',\n",
       " 'removing',\n",
       " '.it',\n",
       " 'is',\n",
       " 'very',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'to',\n",
       " 'be',\n",
       " 'done',\n",
       " 'after',\n",
       " 'tokenization']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "463a33fb-02e2-43c4-a3d5-5a3a7f5b4d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example\n",
      "demonstrate\n",
      "stop\n",
      "word\n",
      "removing\n",
      ".it\n",
      "useful\n",
      "analysis\n",
      "done\n",
      "tokenization\n"
     ]
    }
   ],
   "source": [
    "for w in word:\n",
    "    if w.lower() not in stop_w:\n",
    "        print(w)\n",
    "                # only important word aiya hai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "129b75ce-a2d4-426a-8c01-6b72bc4ec26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'to',\n",
       " 'demonstrate',\n",
       " 'stop',\n",
       " 'word',\n",
       " 'removing',\n",
       " '.it',\n",
       " 'is',\n",
       " 'very',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'to',\n",
       " 'be',\n",
       " 'done',\n",
       " 'after',\n",
       " 'tokenization']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hamesa stopword ho hatana jaruri hota hai \n",
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe1075-4136-4dc1-879c-5a98e8f5d637",
   "metadata": {},
   "source": [
    "\n",
    "# part of speech tagging (pos) >> it is the process of labbeling each word in sentence with grametical role >> noune pronoun adjetive  etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "971140ad-0e26-47e3-89cc-eb6c278f873a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\rk318\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "89a5e475-9240-45ce-843b-c1224b0f4281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('example', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('demonstrate', 'VB'),\n",
       " ('stop', 'JJ'),\n",
       " ('word', 'NN'),\n",
       " ('removing', 'VBG'),\n",
       " ('.it', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('very', 'RB'),\n",
       " ('useful', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('analysis', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('done', 'VBN'),\n",
       " ('after', 'IN'),\n",
       " ('tokenization', 'NN')]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "pos_tag(word)\n",
    "\n",
    "\n",
    "\"\"\"| Word         | POS Tag | Meaning                                 |\n",
    "| ------------ | ------- | --------------------------------------- |\n",
    "| this         | DT      | Determiner (yeh, wo, ek, etc.)          |\n",
    "| is           | VBZ     | Verb, 3rd person singular present       |\n",
    "| an           | DT      | Determiner                              |\n",
    "| example      | NN      | Noun, singular common noun              |\n",
    "| to           | TO      | to (infinitive marker)                |\n",
    "| demonstrate  | VB      | Verb, base form                         |  # parts of speech \n",
    "| stop         | JJ      | Adjective                               |\n",
    "| word         | NN      | Noun, singular                          |\n",
    "| removing     | VBG     | Verb, gerund/present participle         |\n",
    "| .it          | NNP     | Proper noun, singular                   |\n",
    "| is           | VBZ     | Verb, 3rd person singular present       |\n",
    "| very         | RB      | Adverb                                  |\n",
    "| useful       | JJ      | Adjective                               |\n",
    "| for          | IN      | Preposition / subordinating conjunction |\n",
    "| analysis     | NN      | Noun, singular                          |\n",
    "| and          | CC      | Coordinating conjunction (and, or, but) |\n",
    "| to           | TO      | to (infinitive marker)                |\n",
    "| be           | VB      | Verb, base form                         |\n",
    "| done         | VBN     | Verb, past participle                   |\n",
    "| after        | IN      | Preposition / subordinating conjunction |\n",
    "| tokenization | NN      | Noun, singular                          |\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "00a9c48d-951d-4554-8234-28042aeec664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('running', 'VBG')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag([\"running\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a8f3d-157d-4929-a74c-6d1c72265242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
